# RAGFastAPIChatBot

**RAGFastAPIChatBot** is a smart **Retrieval-Augmented Generation (RAG) API** with a **Streamlit frontend** that allows you to ask questions across various document types, including PDFs, Word files, text files, images, CSVs, and small databases. It combines **document ingestion, vector embeddings, OCR processing, and LLMs** to provide accurate, context-aware answers.

## Features
- **Multi-format Document Support:** `.pdf`, `.docx`, `.txt`, `.jpg`, `.png`, `.csv`, `.db`
- **OCR Support:** Extract text from scanned images and PDFs using `pytesseract` with preprocessing
- **Vector Embeddings & Search:** Embeddings generated with `SentenceTransformers` and stored in **FAISS** for efficient similarity search
- **RAG Queries:** Context-aware prompt construction for LLMs, returns concise answers with source metadata
- **API Endpoints:** `POST /upload` – Upload documents, `POST /query` – Ask questions with optional base64 images
- Streamlit frontend for interactive use
- Multi-document querying support
- Dockerized deployment
- Handles both document-based and image-based queries

## Installation, Running, Usage, and Everything Else

# 1. Clone the repository
```
git clone https://github.com/yourusername/RAGFastAPIChatBot.git
cd RAGFastAPIChatBot
```

# 2. Create virtual environment
```
python -m venv venv
```

# 3. Activate virtual environment
```
# Linux/macOS
source venv/bin/activate
```
```
# Windows
venv\Scripts\activate
```

# 4. Install dependencies
```
pip install --upgrade pip
pip install -r requirements.txt
```

# 5. Add OpenAI API key
```
echo "OPENAI_API_KEY=your_openai_api_key_here" > .env
```

# 6. Run FastAPI server
```
uvicorn rag_api.app:app --reload
```
- Preview
```
# API URL: http://127.0.0.1:8000
# Endpoints: /upload – Upload documents, /query – Ask questions
```

# 7. Run Streamlit Frontend
### Make sure you are in the project root and the virtual environment is activated
```
streamlit run app_frontend.py
```
- This will open a local URL like http://localhost:8501
```
# . Using the Streamlit UI
# - Upload PDFs, Word files, images, CSVs, or small databases
# - Enter your question in the input box
# - The app will display:
#   - The answer generated by the LLM
#   - Context from relevant document chunks
#   - Source filenames and chunk indexes
#   - OCR text if an image was used
```

# 8. Run using Docker Compose (optional)
```
docker-compose up --build
```
- Preview
```
# Upload Files: POST /upload (PDF, DOCX, TXT, JPG, PNG, CSV, DB)
# Ask a Question: POST /query
curl -X POST "http://127.0.0.1:8000/query" \
-H "Content-Type: application/json" \
-d '{
  "question": "What are the payment terms in the invoice?",
  "image_base64": "optional_base64_encoded_image"
}'
```

# 9. Example Response:
```
# {
#   "answer": "LLM-generated answer",
#   "context": "Retrieved document chunks",
#   "sources": [{"filename": "invoice.pdf", "chunk_index": 0}],
#   "ocr_text": "Text extracted from image (if any)"
# }
```

## Technologies used:
- Backend: FastAPI, Python 3.11
- Embeddings: SentenceTransformers (all-MiniLM-L6-v2)
- Vector DB: FAISS
- OCR: pytesseract, OpenCV preprocessing
- LLM: OpenAI GPT-4o-mini
- Frontend: Streamlit
- Containerization: Docker


# Notes:
- Documents are chunked (default 500 tokens, 100 overlap) for context relevance
- OCR preprocessing includes grayscale conversion, thresholding, dilation/erosion for better recognition
- If no relevant information is found, response is:
###   "Information not available in the provided documents."

# Contributing:
### Contributions are welcome! Please open an issue or submit a pull request for improvements, bug fixes, or new features.

# License:
### MIT License. See LICENSE file for details.
